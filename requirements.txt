# requirements.txt
requests==2.31.0
beautifulsoup4==4.12.2
gspread==5.10.0
google-auth==2.23.0
google-auth-oauthlib==1.0.0
google-auth-httplib2==0.1.0
lxml==4.9.3

# cron_scheduler.py
import os
import subprocess
from datetime import datetime
import pytz

def run_scraper():
    """Run the web scraper script"""
    try:
        # Set IST timezone
        ist = pytz.timezone('Asia/Kolkata')
        current_time = datetime.now(ist)
        
        print(f"Running scraper at {current_time.strftime('%Y-%m-%d %H:%M:%S IST')}")
        
        # Run the main scraper script
        result = subprocess.run(['python', 'web_scraper.py'], 
                              capture_output=True, text=True)
        
        if result.returncode == 0:
            print("Scraper completed successfully")
            print(result.stdout)
        else:
            print("Scraper failed")
            print(result.stderr)
            
    except Exception as e:
        print(f"Error running scraper: {e}")

if __name__ == "__main__":
    run_scraper()

# Dockerfile (for containerized deployment)
FROM python:3.9-slim

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    cron \
    && rm -rf /var/lib/apt/lists/*

# Copy requirements and install Python dependencies
COPY requirements.txt .
RUN pip install -r requirements.txt

# Copy application files
COPY . .

# Setup cron job
RUN echo "0 11,18 * * * cd /app && /usr/local/bin/python cron_scheduler.py >> /var/log/scraper.log 2>&1" > /etc/cron.d/scraper-cron
RUN chmod 0644 /etc/cron.d/scraper-cron
RUN crontab /etc/cron.d/scraper-cron

# Create log file
RUN touch /var/log/scraper.log

CMD ["cron", "-f"]

# docker-compose.yml
version: '3.8'

services:
  web-scraper:
    build: .
    volumes:
      - ./service_account.json:/app/service_account.json:ro
      - ./logs:/var/log
    environment:
      - GOOGLE_CREDENTIALS_PATH=/app/service_account.json
    restart: unless-stopped

# Railway deployment (railway.json)
{
  "$schema": "https://railway.app/railway.toml.schema.json",
  "build": {
    "builder": "dockerfile"
  },
  "deploy": {
    "restartPolicyType": "always"
  }
}

# GitHub Actions Workflow (.github/workflows/scraper.yml)
name: Web Scraper

on:
  schedule:
    # Runs at 11:00 AM and 6:00 PM IST (5:30 AM and 12:30 PM UTC)
    - cron: '30 5,12 * * *'
  workflow_dispatch: # Allows manual trigger

jobs:
  scrape:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
    
    - name: Install dependencies
      run: |
        pip install -r requirements.txt
    
    - name: Create service account file
      env:
        GOOGLE_CREDENTIALS: ${{ secrets.GOOGLE_CREDENTIALS }}
      run: |
        echo "$GOOGLE_CREDENTIALS" > service_account.json
    
    - name: Run scraper
      run: |
        python web_scraper.py
